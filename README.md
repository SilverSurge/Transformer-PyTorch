# Transformer-PyTorch

In this repository, I recreate the transformer architecture which was introduced in 'Attention is All You Need" Paper in 2017. I have slightly modified the architecture like including a learnable positional embedding and including dropout layers to prevent overfitting.


All of the classes can be found in `transformer.py` file. 